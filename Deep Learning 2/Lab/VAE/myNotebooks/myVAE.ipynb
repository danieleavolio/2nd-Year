{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempio mio di Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innanzitutto diciamo che un Autoencoder è una particolare architettura delle reti neurali che prevede l'utilizzo di due\n",
    "modelli precisi.\n",
    "  \n",
    "- Encoder\n",
    "- Decoder\n",
    "\n",
    "L'encoder è un modello che prende in input un vettore di dati e lo trasforma in un vettore di dimensione inferiore.\n",
    "Il decoder invece prende in input il vettore di dimensione inferiore e lo trasforma in un vettore di dimensione superiore.\n",
    "\n",
    "L'obiettivo è quello di fare in modo che il vettore di dimensione inferiore sia una rappresentazione compressa del vettore\n",
    "di dimensione superiore e che il decoder sia in grado di ricostruire il vettore di dimensione superiore a partire dal\n",
    "vettore di dimensione inferiore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemi negli Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il problema principale degli autoencoder è che la rappresentazione compressa del vettore di dimensione superiore non è\n",
    "necessariamente una rappresentazione significativa. Infatti, se l'encoder e il decoder sono due modelli lineari, la\n",
    "rappresentazione compressa sarà una combinazione lineare dei dati di input. Questo significa che se i dati di input\n",
    "non sono linearmente indipendenti, la rappresentazione compressa non sarà significativa.\n",
    "\n",
    "Ad esempio, se consideriamo ciò che viene fuori dall'encoder come **spazio latente**, due punti che sono vicini all'interno \n",
    "dello spazio latente non sono necessariamente vicini all'interno dello spazio originale. \n",
    "\n",
    "Il problema è che i dati non seguono una distribuzione normale, ma seguono una distribuzione che è molto più complessa\n",
    "di una distribuzione normale. Questo significa che non è possibile utilizzare un encoder lineare per comprimere i dati\n",
    "in un vettore di dimensione inferiore e ottenere una rappresentazione significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE\n",
    "\n",
    "I variational autoencoder risolvono questo problema.\n",
    "\n",
    "## Riduzione della dimensionalità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VAE](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*UdOybs9wOe3zW8vDAfj9VA@2x.png)\n",
    "\n",
    "Principalmente ci basiamo sul concetto di PCA (Principal Component Analysis) che è un algoritmo di riduzione della\n",
    "dimensione che seleziona le feature più significative di un dataset. Praticamente è il task che viene fatto\n",
    "dall'encoder.\n",
    "\n",
    "### Come funziona la PCA\n",
    "\n",
    "Consideriamo dei punti all'interno dello spazio latente, la PCA punta a voler generare **nuovi punti** partendo dai precedenti, ma lo fa in modo \n",
    "intelligente. Praticamente prende dei punti nello spazio latente che sono combinazione lineare dei vecchi punti tale che la proiezione\n",
    "dei vecchi punti nel sottospazio definitio dalle nuove feature è la minore possibile.\n",
    "\n",
    "![PCA](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ayo0n2zq_gy7VERYmp4lrA@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE in gioco\n",
    "\n",
    "Il motivo principale per cui i VAE sono così potenti è che non sono limitati a generare nuovi punti nello spazio latente. Al contrario, due punti come detto prima nello spazio latente degli autoencoder base potrebbero essere punti senza senso. Come funzionano quindi?\n",
    "\n",
    "\n",
    "Entrano in gioco i **regularizers** durante la fase di addestramento. \n",
    "\n",
    "Non solo, però. Oltre ad introdurre questo, **non consideriamo l'encoding dell'input come singolo punto**, bensì lo facciamo come **una distribuzione sullo spazio latente**\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Si fa encoding del punto come una distribuzione\n",
    "2. Si fa sampling dalla distribuzione e si prende un punto\n",
    "3. Il punto viene decodificato\n",
    "4. Si calcola l'errore per allenare la rete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La parte di regolarizzazione\n",
    "\n",
    "\n",
    "I punti devono essere **continui** cioè due punti vicini devono essere simili e **completi**, cioè un punto estratto da una distribuzione deve essere effettivamente con del contenuto valido, e non un punto senza senso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La loss\n",
    "\n",
    "\n",
    "La funzione di loss per calcolare l'erorre sfrutta la **KL divergence** che è una misura di quanto due distribuzioni siano simili.\n",
    "\n",
    "$$ \\sum_i^n \\sigma^2 + \\mu^2 - \\log \\sigma - 1 $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
