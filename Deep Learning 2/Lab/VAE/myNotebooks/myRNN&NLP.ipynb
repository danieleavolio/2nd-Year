{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Le **recurrent neural netowrk** sono utilizzato quando i dati che abbiamo in input sono dati dipendenti in modo sequenziale. Nel senso, se il \n",
    "dato $2$ è dipendente dal dato $1$, allora il dato $3$ sarà dipendente dal dato $2$ e così via. Ok, in questi casi \n",
    "non si utilizzano le **FFN** ma le **RNN**\n",
    "\n",
    "![rnn](https://machinelearningmastery.com/wp-content/uploads/2021/09/rnn1-1024x484.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praticamente, vediamo che $x_t$ sarebbe l'input al tempo $t$, $h_t$ possiamo chiamarlo il **contesto corrente**, cioè dipende dal valore delle unità nascoste al tempo $t$. $y$ ovviamente è l'output, con $t$ per indicare al tempo $t$.\n",
    "\n",
    "La rappresentazione di sotto, invece, mostra il processo in modo lineare. Questo è il concetto base delle reti neurali ricorrenti.\n",
    "\n",
    "**Nota:** L'ultimo livello è un livello di feedforward per ottenere l'output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori nascosti $h$ si calcolano in questi modo:\n",
    "\n",
    "$$ h_t+1 = f(x_t, h_t, w_x, w_h, b_h) = f(w_x x_t + w_h h_t + b_h) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'output invce:\n",
    "\n",
    "$$y_t = f(h_t, w_y) = f(w_y h_t + b_y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quali funzioni di attivazione si usano?\n",
    "\n",
    "- **TanH**\n",
    "- **Sigmoid**\n",
    "- **ReLU**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipi di RNN\n",
    "\n",
    "- **One to One**: pratiacmente si tratta di una FFN\n",
    "- **One to Many**: si tratta di una RNN che prende in input un solo dato e restituisce una sequenza di dati. Si usa nella generazione di musica, ad esempio.\n",
    "- **Many to One**: Si usa nella sentiment analysis, dove si prende un testo formato da tante parole e poi si vuole tirare fuori una label da questo\n",
    "- **Many to Many**: Si usano per la machine translation\n",
    "\n",
    "### Problemi\n",
    "\n",
    "- Molto lenta\n",
    "- Non impara robe nuove\n",
    "- Sparizione del gradiente: se il gradiente è molto piccolo, allora il modello non impara più niente. Questo è un problema che si risolve con le **LSTM**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "Le LSTM risolvono il problema della **sparizione del gradiente**.\n",
    "\n",
    "Ad esempio, la frase \"Io sono della francia, ... , parlo benissimo il francese\". L'informazione che sono della Francia per le RNN sarebbe difficile da mantenere perché si trova più indietro nel discorso. Nelle LSTM, questo non avviene.\n",
    "\n",
    "\n",
    "Struttura di una RNN:\n",
    "\n",
    "![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "\n",
    "Struttura di una LSTM:\n",
    "\n",
    "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "![OPERAZIONI](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo ogni parte della LSTM per capire come funziona.\n",
    "\n",
    "**Nota: $C_{t-1}$ e $C_t$ sarebbe lo stato della cella**\n",
    "\n",
    "**Parte di sopra**: Il **sigmoid layer** $\\sigma$ praticamente da in output un numero da 0 a 1 e aiuta a dire **quanto deve passare** di informazione al prossimo layer. Con $0$ indica CHE NON PASSA NIENTE, con $1$ indica che PASSA TUTTO. Questo si chiama il **forget gate**.\n",
    "\n",
    "Analizziamo ora il workflow in basso. Praticamente, pensiamo ad un esempio in cui vogliamo fare predizioni di parole.\n",
    "\n",
    "Se vogliamo usare i pronomi corretti per una persona sapendo il suo sesso, se ne leggiamo un'altra di persona dobbiamo usare i pronomi corretti per l'altra persona.\n",
    "\n",
    "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "\n",
    "Il prossimo livello si occupa di capire quali dati dare in input. Il primo è un input gate layer $\\sigma$ e poi abbiamo $tanh$ layer che praticamente \n",
    "ci da un vettore di che contiene valori possibili da assegnare al prossimo livello. \n",
    "\n",
    "![aaa](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "Ora per aggiornare la cella $C_t$ basta moltiplicare il vettore $i_t$ per il vettore $C_t$ e sommare il risultato al vettore $f_t$ per il vettore $C_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adeso praticamente manca solamente cosa outputtare.\n",
    "\n",
    "![aaa](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "Si utilizza un'altra $\\sigma$ sigmoid che decide quale parte dello stato della cella si andrà a outputtare, si fa passare anche lo stato da una $tanh$ per ottenere valori tra $-1$ e $1$ e si moltiplicano questi valori, per ottenere il vettore di output.\n",
    "\n",
    "Questo è il funzionamento diciamo delle reti LSTM. Ovviamente, ci sono tanti altri tipi di LSTM, ma questo è il funzionamento base.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
