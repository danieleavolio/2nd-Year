\contentsline {section}{\numberline {1}Introduzione}{6}{section.1}%
\contentsline {section}{\numberline {2}Deep Learning 101}{6}{section.2}%
\contentsline {subsection}{\numberline {2.1}Architetture e strumenti nel deep learning}{6}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Libri utili}{6}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Strumenti che useremo}{7}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Schema generale di un problema di deep learning}{7}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Perché si usa il termine "Tensore"?}{7}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}AI vs DL}{8}{subsection.2.6}%
\contentsline {section}{\numberline {3}Introduzione alle Reti Neurali}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}Il modello di McCulloch-Pitts}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Modello di Rosenblatt}{9}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Esempio con 2 neuroni}{10}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Rappresentazione le funzioni logiche}{11}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}AND}{11}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Il problema dello XOR}{12}{subsubsection.3.4.2}%
\contentsline {subsection}{\numberline {3.5}Gli ingredienti di una rete neurale}{13}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Il grafo g}{13}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}La funzione di loss}{14}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Funzione di loss per Regressione}{15}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Funzione di loss per classificazione}{15}{subsubsection.3.5.4}%
\contentsline {subsection}{\numberline {3.6}L'ottimizzatore o}{15}{subsection.3.6}%
\contentsline {subsubsection}{\numberline {3.6.1}Il metodo di discesa del gradiente}{16}{subsubsection.3.6.1}%
\contentsline {subsubsection}{\numberline {3.6.2}Il metodo di discesa del gradiente stocastico}{17}{subsubsection.3.6.2}%
\contentsline {subsubsection}{\numberline {3.6.3}Linee guida sul learning rate}{18}{subsubsection.3.6.3}%
\contentsline {section}{\numberline {4}Classificazione Binaria}{20}{section.4}%
\contentsline {subsection}{\numberline {4.1}Caricamento e gestione del Dataset}{21}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Definizione della Rete Neurale}{22}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Plotting del modello}{24}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Training del modello e valutazione}{25}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Validation Set}{25}{subsubsection.4.4.1}%
\contentsline {subsection}{\numberline {4.5}Prediction}{29}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Come risolvere problemi di accuracy bassa?}{29}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Early Stopping}{30}{subsection.4.7}%
\contentsline {section}{\numberline {5}Classificazione Multiclasse}{30}{section.5}%
\contentsline {subsection}{\numberline {5.1}Descrizione del dataset - Reuters}{30}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Come processiamo l'output?}{30}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Softmax activation function}{31}{subsubsection.5.1.2}%
\contentsline {section}{\numberline {6}Regressione}{33}{section.6}%
\contentsline {subsection}{\numberline {6.1}Boston Housing Price}{33}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Normalizzare i dati}{33}{subsubsection.6.1.1}%
\contentsline {subsubsection}{\numberline {6.1.2}Costruzione della rete}{34}{subsubsection.6.1.2}%
\contentsline {subsubsection}{\numberline {6.1.3}Validation con pochi data point}{34}{subsubsection.6.1.3}%
\contentsline {subsubsection}{\numberline {6.1.4}Visualizzare i risultati}{35}{subsubsection.6.1.4}%
\contentsline {subsection}{\numberline {6.2}Overfitting}{35}{subsection.6.2}%
\contentsline {subsubsection}{\numberline {6.2.1}Regolarizzazione}{35}{subsubsection.6.2.1}%
\contentsline {subsection}{\numberline {6.3}Dropout}{37}{subsection.6.3}%
\contentsline {section}{\numberline {7}Convolutional Neural Networks}{38}{section.7}%
\contentsline {subsection}{\numberline {7.1}Il primo problema con le immagini}{38}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Il secondo problema}{38}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Come fa la rete a riconoscere i pattern}{38}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Convoluzione}{39}{subsection.7.4}%
\contentsline {subsection}{\numberline {7.5}Padding}{40}{subsection.7.5}%
\contentsline {subsection}{\numberline {7.6}Strides}{40}{subsection.7.6}%
\contentsline {subsection}{\numberline {7.7}Esempio 1}{40}{subsection.7.7}%
\contentsline {subsection}{\numberline {7.8}Pooling}{41}{subsection.7.8}%
\contentsline {subsubsection}{\numberline {7.8.1}Max Pooling}{41}{subsubsection.7.8.1}%
\contentsline {subsection}{\numberline {7.9}Multiclass Classification Example}{42}{subsection.7.9}%
\contentsline {subsection}{\numberline {7.10}Nozioni alla lavagna}{43}{subsection.7.10}%
\contentsline {section}{\numberline {8}Reti Neurali Convoluzionali Pre-allenate}{44}{section.8}%
\contentsline {subsection}{\numberline {8.1}Come si usa il transfer learning?}{44}{subsection.8.1}%
\contentsline {section}{\numberline {9}Oltre il modello sequenziale}{46}{section.9}%
\contentsline {subsection}{\numberline {9.1}Multi input e multi output}{46}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Functional API}{46}{subsection.9.2}%
\contentsline {section}{\numberline {10}Adanvced Keras}{48}{section.10}%
\contentsline {subsection}{\numberline {10.1}Subclassing}{48}{subsection.10.1}%
\contentsline {section}{\numberline {11}Serie Temporali | Time Series}{49}{section.11}%
\contentsline {subsection}{\numberline {11.1}Recurrent Neural Network RNN}{49}{subsection.11.1}%
\contentsline {subsubsection}{\numberline {11.1.1}L'utilizzo delle RNN}{50}{subsubsection.11.1.1}%
\contentsline {subsection}{\numberline {11.2}Long Short Term Memory (LSTM)}{52}{subsection.11.2}%
\contentsline {section}{\numberline {12}Autoencoders, Transformer}{56}{section.12}%
\contentsline {subsection}{\numberline {12.1}Cosa si può fare con gli autoencoder?}{56}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}Accenno su Style Transfer}{58}{subsection.12.2}%
\contentsline {subsection}{\numberline {12.3}Accenno su Stable Diffusion}{58}{subsection.12.3}%
\contentsline {subsection}{\numberline {12.4}Autoencoder per le sequenze}{59}{subsection.12.4}%
\contentsline {subsubsection}{\numberline {12.4.1}Seq2Seq}{59}{subsubsection.12.4.1}%
\contentsline {subsubsection}{\numberline {12.4.2}Training di Seq2Seq}{59}{subsubsection.12.4.2}%
\contentsline {subsubsection}{\numberline {12.4.3}Problemi di questa architettura}{60}{subsubsection.12.4.3}%
\contentsline {subsection}{\numberline {12.5}Da Seq2Seq ad accenni Transformer}{60}{subsection.12.5}%
\contentsline {subsubsection}{\numberline {12.5.1}Cosa significa Masking?}{62}{subsubsection.12.5.1}%
\contentsline {section}{\numberline {13}Architettura Transformer}{64}{section.13}%
\contentsline {subsection}{\numberline {13.1}Multihead Attention}{65}{subsection.13.1}%
\contentsline {subsection}{\numberline {13.2}Positional Encoding}{69}{subsection.13.2}%
\contentsline {subsection}{\numberline {13.3}Come si allenano i transformers?}{70}{subsection.13.3}%
\contentsline {subsection}{\numberline {13.4}In Context Learning}{71}{subsection.13.4}%
\contentsline {subsection}{\numberline {13.5}Few Shots learning}{71}{subsection.13.5}%
\contentsline {section}{\numberline {14}Lab: Introduzione Python}{72}{section.14}%
\contentsline {subsection}{\numberline {14.1}MatPlotLib}{72}{subsection.14.1}%
\contentsline {subsubsection}{\numberline {14.1.1}Plots}{72}{subsubsection.14.1.1}%
\contentsline {subsubsection}{\numberline {14.1.2}Sub-plots}{72}{subsubsection.14.1.2}%
\contentsline {subsection}{\numberline {14.2}NumPy}{73}{subsection.14.2}%
\contentsline {section}{\numberline {15}Lab: Reti Neurali da zero}{75}{section.15}%
\contentsline {subsection}{\numberline {15.1}Introduzione}{75}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Esempio pratico}{75}{subsection.15.2}%
\contentsline {subsubsection}{\numberline {15.2.1}Il grafo}{76}{subsubsection.15.2.1}%
\contentsline {subsubsection}{\numberline {15.2.2}La funnzione loss}{76}{subsubsection.15.2.2}%
\contentsline {subsubsection}{\numberline {15.2.3}L'ottimizzatore}{77}{subsubsection.15.2.3}%
\contentsline {subsubsection}{\numberline {15.2.4}Discesa del gradiente}{77}{subsubsection.15.2.4}%
\contentsline {subsubsection}{\numberline {15.2.5}Inizializzatore}{77}{subsubsection.15.2.5}%
\contentsline {subsubsection}{\numberline {15.2.6}Fix Point Procedure}{77}{subsubsection.15.2.6}%
\contentsline {subsection}{\numberline {15.3}Esempio da zero}{77}{subsection.15.3}%
\contentsline {subsubsection}{\numberline {15.3.1}La funzione Sigmoid}{77}{subsubsection.15.3.1}%
\contentsline {subsection}{\numberline {15.4}Tangente Iperbolica TanH}{78}{subsection.15.4}%
\contentsline {subsection}{\numberline {15.5}ReLu}{79}{subsection.15.5}%
\contentsline {subsection}{\numberline {15.6}Scalare i valori}{80}{subsection.15.6}%
\contentsline {subsection}{\numberline {15.7}Plottare la loss}{81}{subsection.15.7}%
\contentsline {subsection}{\numberline {15.8}Importante cosa su Gradient Descent}{82}{subsection.15.8}%
\contentsline {section}{\numberline {16}TensorFlow e Keras}{83}{section.16}%
\contentsline {subsection}{\numberline {16.1}One Hot Encoding}{83}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Variabili correlate}{83}{subsection.16.2}%
\contentsline {subsection}{\numberline {16.3}Accuracy come loss}{83}{subsection.16.3}%
\contentsline {subsection}{\numberline {16.4}Epoche e batch size}{83}{subsection.16.4}%
\contentsline {subsection}{\numberline {16.5}Overfitting e come evitarlo}{84}{subsection.16.5}%
\contentsline {subsection}{\numberline {16.6}Migliorare le performance di un modello}{84}{subsection.16.6}%
\contentsline {section}{\numberline {17}Lab: Convolutional Neural Networks}{85}{section.17}%
\contentsline {subsection}{\numberline {17.1}Cross Entropy vs Accuracy}{85}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}Workflow per image classification}{86}{subsection.17.2}%
\contentsline {subsubsection}{\numberline {17.2.1}Bilanciare le classi}{86}{subsubsection.17.2.1}%
\contentsline {section}{\numberline {18}Autoencoders}{88}{section.18}%
\contentsline {subsection}{\numberline {18.1}Nota su PCA}{88}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Autoencoders}{88}{subsection.18.2}%
\contentsline {subsubsection}{\numberline {18.2.1}Gli steps}{89}{subsubsection.18.2.1}%
\contentsline {subsubsection}{\numberline {18.2.2}Applicazioni}{89}{subsubsection.18.2.2}%
\contentsline {subsection}{\numberline {18.3}Autoencoders e Convolution}{90}{subsection.18.3}%
\contentsline {section}{\numberline {19}VAE (Variational Autoencoder)}{91}{section.19}%
\contentsline {subsection}{\numberline {19.1}Regularization term e Reparametrization trick}{92}{subsection.19.1}%
\contentsline {section}{\numberline {20}Recurrent Neural Network e Natural Language Processing (RNN e NLP)}{95}{section.20}%
\contentsline {subsection}{\numberline {20.1}RNN e LSTM}{95}{subsection.20.1}%
\contentsline {subsection}{\numberline {20.2}NLP}{97}{subsection.20.2}%
\contentsline {subsection}{\numberline {20.3}Come si lavora con i Word Embedding?}{97}{subsection.20.3}%
\contentsline {subsubsection}{\numberline {20.3.1}Encoder}{97}{subsubsection.20.3.1}%
