\contentsline {section}{\numberline {1}Introduzione}{6}{section.1}%
\contentsline {section}{\numberline {2}Deep Learning 101}{6}{section.2}%
\contentsline {subsection}{\numberline {2.1}Architetture e strumenti nel deep learning}{6}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Libri utili}{6}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Strumenti che useremo}{7}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Schema generale di un problema di deep learning}{7}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Perché si usa il termine "Tensore"?}{7}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}AI vs DL}{8}{subsection.2.6}%
\contentsline {section}{\numberline {3}Introduzione alle Reti Neurali}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}Il modello di McCulloch-Pitts}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Modello di Rosenblatt}{9}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Esempio con 2 neuroni}{10}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Rappresentazione le funzioni logiche}{11}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}AND}{11}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Il problema dello XOR}{12}{subsubsection.3.4.2}%
\contentsline {subsection}{\numberline {3.5}Gli ingredienti di una rete neurale}{13}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Il grafo g}{13}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}La funzione di loss}{14}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Funzione di loss per Regressione}{15}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Funzione di loss per classificazione}{15}{subsubsection.3.5.4}%
\contentsline {subsection}{\numberline {3.6}L'ottimizzatore o}{15}{subsection.3.6}%
\contentsline {subsubsection}{\numberline {3.6.1}Il metodo di discesa del gradiente}{16}{subsubsection.3.6.1}%
\contentsline {subsubsection}{\numberline {3.6.2}Il metodo di discesa del gradiente stocastico}{17}{subsubsection.3.6.2}%
\contentsline {subsubsection}{\numberline {3.6.3}Linee guida sul learning rate}{18}{subsubsection.3.6.3}%
\contentsline {section}{\numberline {4}Classificazione Binaria}{20}{section.4}%
\contentsline {subsection}{\numberline {4.1}Caricamento e gestione del Dataset}{21}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Definizione della Rete Neurale}{22}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Plotting del modello}{24}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Training del modello e valutazione}{25}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Validation Set}{25}{subsubsection.4.4.1}%
\contentsline {subsection}{\numberline {4.5}Prediction}{29}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Come risolvere problemi di accuracy bassa?}{29}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Early Stopping}{30}{subsection.4.7}%
\contentsline {section}{\numberline {5}Classificazione Multiclasse}{30}{section.5}%
\contentsline {subsection}{\numberline {5.1}Descrizione del dataset - Reuters}{30}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Come processiamo l'output?}{30}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Softmax activation function}{31}{subsubsection.5.1.2}%
\contentsline {section}{\numberline {6}Regressione}{33}{section.6}%
\contentsline {subsection}{\numberline {6.1}Boston Housing Price}{33}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Normalizzare i dati}{33}{subsubsection.6.1.1}%
\contentsline {subsubsection}{\numberline {6.1.2}Costruzione della rete}{34}{subsubsection.6.1.2}%
\contentsline {subsubsection}{\numberline {6.1.3}Validation con pochi data point}{34}{subsubsection.6.1.3}%
\contentsline {subsubsection}{\numberline {6.1.4}Visualizzare i risultati}{35}{subsubsection.6.1.4}%
\contentsline {subsection}{\numberline {6.2}Overfitting}{35}{subsection.6.2}%
\contentsline {subsubsection}{\numberline {6.2.1}Regolarizzazione}{35}{subsubsection.6.2.1}%
\contentsline {subsection}{\numberline {6.3}Dropout}{37}{subsection.6.3}%
\contentsline {section}{\numberline {7}Convolutional Neural Networks}{38}{section.7}%
\contentsline {subsection}{\numberline {7.1}Il primo problema con le immagini}{38}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Il secondo problema}{38}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Come fa la rete a riconoscere i pattern}{38}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Convoluzione}{39}{subsection.7.4}%
\contentsline {subsection}{\numberline {7.5}Padding}{40}{subsection.7.5}%
\contentsline {subsection}{\numberline {7.6}Strides}{40}{subsection.7.6}%
\contentsline {subsection}{\numberline {7.7}Esempio 1}{40}{subsection.7.7}%
\contentsline {subsection}{\numberline {7.8}Pooling}{41}{subsection.7.8}%
\contentsline {subsubsection}{\numberline {7.8.1}Max Pooling}{41}{subsubsection.7.8.1}%
\contentsline {subsection}{\numberline {7.9}Multiclass Classification Example}{42}{subsection.7.9}%
\contentsline {subsection}{\numberline {7.10}Nozioni alla lavagna}{43}{subsection.7.10}%
\contentsline {section}{\numberline {8}Reti Neurali Convoluzionali Pre-allenate}{44}{section.8}%
\contentsline {subsection}{\numberline {8.1}Come si usa il transfer learning?}{44}{subsection.8.1}%
\contentsline {section}{\numberline {9}Oltre il modello sequenziale}{46}{section.9}%
\contentsline {subsection}{\numberline {9.1}Multi input e multi output}{46}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Functional API}{46}{subsection.9.2}%
\contentsline {section}{\numberline {10}Adanvced Keras}{48}{section.10}%
\contentsline {subsection}{\numberline {10.1}Subclassing}{48}{subsection.10.1}%
\contentsline {section}{\numberline {11}Serie Temporali | Time Series}{49}{section.11}%
\contentsline {subsection}{\numberline {11.1}Recurrent Neural Network RNN}{49}{subsection.11.1}%
\contentsline {subsubsection}{\numberline {11.1.1}L'utilizzo delle RNN}{50}{subsubsection.11.1.1}%
\contentsline {subsection}{\numberline {11.2}Long Short Term Memory (LSTM)}{52}{subsection.11.2}%
\contentsline {section}{\numberline {12}Autoencoders, Transformer}{56}{section.12}%
\contentsline {subsection}{\numberline {12.1}Cosa si può fare con gli autoencoder?}{56}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}Accenno su Style Transfer}{58}{subsection.12.2}%
\contentsline {subsection}{\numberline {12.3}Accenno su Stable Diffusion}{58}{subsection.12.3}%
\contentsline {subsection}{\numberline {12.4}Autoencoder per le sequenze}{59}{subsection.12.4}%
\contentsline {subsubsection}{\numberline {12.4.1}Seq2Seq}{59}{subsubsection.12.4.1}%
\contentsline {subsubsection}{\numberline {12.4.2}Training di Seq2Seq}{59}{subsubsection.12.4.2}%
\contentsline {subsubsection}{\numberline {12.4.3}Problemi di questa architettura}{60}{subsubsection.12.4.3}%
\contentsline {subsection}{\numberline {12.5}Da Seq2Seq a Transformer}{60}{subsection.12.5}%
\contentsline {subsubsection}{\numberline {12.5.1}Cosa significa Masking?}{63}{subsubsection.12.5.1}%
\contentsline {section}{\numberline {13}Lab: Introduzione Python}{63}{section.13}%
\contentsline {subsection}{\numberline {13.1}MatPlotLib}{63}{subsection.13.1}%
\contentsline {subsubsection}{\numberline {13.1.1}Plots}{63}{subsubsection.13.1.1}%
\contentsline {subsubsection}{\numberline {13.1.2}Sub-plots}{64}{subsubsection.13.1.2}%
\contentsline {subsection}{\numberline {13.2}NumPy}{65}{subsection.13.2}%
\contentsline {section}{\numberline {14}Lab: Reti Neurali da zero}{66}{section.14}%
\contentsline {subsection}{\numberline {14.1}Introduzione}{66}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Esempio pratico}{66}{subsection.14.2}%
\contentsline {subsubsection}{\numberline {14.2.1}Il grafo}{67}{subsubsection.14.2.1}%
\contentsline {subsubsection}{\numberline {14.2.2}La funnzione loss}{67}{subsubsection.14.2.2}%
\contentsline {subsubsection}{\numberline {14.2.3}L'ottimizzatore}{68}{subsubsection.14.2.3}%
\contentsline {subsubsection}{\numberline {14.2.4}Discesa del gradiente}{68}{subsubsection.14.2.4}%
\contentsline {subsubsection}{\numberline {14.2.5}Inizializzatore}{68}{subsubsection.14.2.5}%
\contentsline {subsubsection}{\numberline {14.2.6}Fix Point Procedure}{68}{subsubsection.14.2.6}%
\contentsline {subsection}{\numberline {14.3}Esempio da zero}{68}{subsection.14.3}%
\contentsline {subsubsection}{\numberline {14.3.1}La funzione Sigmoid}{68}{subsubsection.14.3.1}%
\contentsline {subsection}{\numberline {14.4}Tangente Iperbolica TanH}{69}{subsection.14.4}%
\contentsline {subsection}{\numberline {14.5}ReLu}{70}{subsection.14.5}%
\contentsline {subsection}{\numberline {14.6}Scalare i valori}{71}{subsection.14.6}%
\contentsline {subsection}{\numberline {14.7}Plottare la loss}{72}{subsection.14.7}%
\contentsline {subsection}{\numberline {14.8}Importante cosa su Gradient Descent}{73}{subsection.14.8}%
\contentsline {section}{\numberline {15}TensorFlow e Keras}{74}{section.15}%
\contentsline {subsection}{\numberline {15.1}One Hot Encoding}{74}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Variabili correlate}{74}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}Accuracy come loss}{74}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}Epoche e batch size}{74}{subsection.15.4}%
\contentsline {subsection}{\numberline {15.5}Overfitting e come evitarlo}{75}{subsection.15.5}%
\contentsline {subsection}{\numberline {15.6}Migliorare le performance di un modello}{75}{subsection.15.6}%
\contentsline {section}{\numberline {16}Lab: Convolutional Neural Networks}{76}{section.16}%
\contentsline {subsection}{\numberline {16.1}Cross Entropy vs Accuracy}{76}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Workflow per image classification}{77}{subsection.16.2}%
\contentsline {subsubsection}{\numberline {16.2.1}Bilanciare le classi}{77}{subsubsection.16.2.1}%
\contentsline {section}{\numberline {17}Autoencoders}{79}{section.17}%
\contentsline {subsection}{\numberline {17.1}Nota su PCA}{79}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}Autoencoders}{79}{subsection.17.2}%
\contentsline {subsubsection}{\numberline {17.2.1}Gli steps}{80}{subsubsection.17.2.1}%
\contentsline {subsubsection}{\numberline {17.2.2}Applicazioni}{80}{subsubsection.17.2.2}%
\contentsline {subsection}{\numberline {17.3}Autoencoders e Convolution}{81}{subsection.17.3}%
\contentsline {section}{\numberline {18}VAE (Variational Autoencoder)}{82}{section.18}%
\contentsline {subsection}{\numberline {18.1}Regularization term e Reparametrization trick}{83}{subsection.18.1}%
\contentsline {section}{\numberline {19}Recurrent Neural Network e Natural Language Processing (RNN e NLP)}{86}{section.19}%
\contentsline {subsection}{\numberline {19.1}RNN e LSTM}{86}{subsection.19.1}%
\contentsline {subsection}{\numberline {19.2}NLP}{88}{subsection.19.2}%
\contentsline {subsection}{\numberline {19.3}Come si lavora con i Word Embedding?}{88}{subsection.19.3}%
\contentsline {subsubsection}{\numberline {19.3.1}Encoder}{88}{subsubsection.19.3.1}%
